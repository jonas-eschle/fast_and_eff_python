{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "id": "eaa3bd00",
    "origin_pos": 1,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GPUs and TPUs\n",
    "\n",
    "We will discuss how\n",
    "to use a single NVIDIA GPU and a Google TPU for calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "id": "77df419c",
    "origin_pos": 8,
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "id": "2d31e075",
    "origin_pos": 9,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Computing Devices\n",
    "\n",
    "We can specify devices, such as CPUs and GPUs,\n",
    "for storage and calculation.\n",
    "By default, tensors are created in the main memory\n",
    "and then the CPU is used for calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "id": "c2b86c09",
    "origin_pos": 13,
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cpu():\n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return jax.devices('cpu')[0]\n",
    "\n",
    "def gpu(i=0):\n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    return jax.devices('gpu')[i]\n",
    "\n",
    "def tpu(i=0):\n",
    "    \"\"\"Get a TPU device.\"\"\"\n",
    "    return jax.devices('tpu')[i]\n",
    "\n",
    "# cpu(), gpu(), gpu(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "id": "2090254b",
    "origin_pos": 14,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can query the number of available GPUs and TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "657c55ac",
    "origin_pos": 16,
    "outputId": "bf6d5201-76d7-4589-c156-1b7cc0ef8754",
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_gpus():\n",
    "    \"\"\"Get the number of available GPUs.\"\"\"\n",
    "    try:\n",
    "        return jax.device_count('gpu')\n",
    "    except:\n",
    "        return 0  # No GPU backend found\n",
    "\n",
    "num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "QeG_bvfO9th7",
    "outputId": "8cd518ee-d60a-4945-bf8e-4e81b22ca84f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tpu():\n",
    "    \"\"\"Get the number of available GPUs.\"\"\"\n",
    "    try:\n",
    "        return jax.device_count('tpu')\n",
    "    except:\n",
    "        return 0  # No GPU backend found\n",
    "num_tpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "editable": true,
    "id": "2a85ad8f",
    "origin_pos": 17,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we define two convenient functions that allow us\n",
    "to run code even if the requested GPUs or TPUs do not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "c6a4e02b",
    "origin_pos": 18,
    "outputId": "670a4489-bb40-4be2-d288-821dcb489aa7",
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_tpu_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if num_tpu() >= i + 1:\n",
    "        return tpu(i)\n",
    "    elif num_gpus() >= i + 1:\n",
    "        return gpu(i)\n",
    "    return cpu()\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    return [gpu(i) for i in range(num_gpus())]\n",
    "\n",
    "try_tpu_gpu(), try_tpu_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "editable": true,
    "id": "ad62dca2",
    "origin_pos": 19,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tensors and GPUs/TPUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "editable": true,
    "id": "debb5a24",
    "origin_pos": 22,
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "source": [
    "By default, tensors are created on the GPU/TPU if they are available,\n",
    "else CPU is used if not available.\n",
    "We can query the device where the tensor is located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "c8e19f73",
    "origin_pos": 26,
    "outputId": "a8947e31-cb15-4cb1-8ed7-42d19d3d8918",
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = jnp.array([1, 2, 3])\n",
    "x.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "id": "a28d9e23",
    "origin_pos": 27,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "It is important to note that whenever we want\n",
    "to operate on multiple terms,\n",
    "they need to be on the same device.\n",
    "For instance, if we sum two tensors,\n",
    "we need to make sure that both arguments\n",
    "live on the same device---otherwise the framework\n",
    "would not know where to store the result\n",
    "or even how to decide where to perform the computation.\n",
    "\n",
    "### Storage on the GPU\n",
    "\n",
    "There are several ways to store a tensor on the GPU.\n",
    "For example, we can specify a storage device when creating a tensor.\n",
    "Next, we create the tensor variable `X` on the first `gpu`.\n",
    "The tensor created on a GPU only consumes the memory of this GPU.\n",
    "We can use the `nvidia-smi` command to view GPU memory usage.\n",
    "In general, we need to make sure that we do not create data that exceeds the GPU memory limit.\n",
    "\n",
    "The syntax is\n",
    "```\n",
    "x_ondevice = jax.device_put(x, 'device')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "f911de18",
    "origin_pos": 31,
    "outputId": "5ea22d4e-3b61-4009-9935-8a49e5df7a9f",
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default JAX puts arrays to GPUs or TPUs if available\n",
    "X = jax.device_put(jnp.ones((2, 3)), try_tpu_gpu())\n",
    "X, X.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "editable": true,
    "id": "d20c7b95",
    "origin_pos": 32,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Assuming that you have at least two TPUs, the following code will create a random tensor, `Y`, on the second GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "7d206927",
    "origin_pos": 36,
    "outputId": "0edc5d34-4653-41ab-a127-85356c54b264",
    "slideshow": {
     "slide_type": ""
    },
    "tab": [
     "jax"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = jax.device_put(jax.random.uniform(jax.random.PRNGKey(0), (2, 3)),\n",
    "                   try_tpu_gpu(1))\n",
    "Y, Y.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "editable": true,
    "id": "bb9f14c9",
    "origin_pos": 37,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Copying\n",
    "\n",
    "If we want to compute `X + Y`,\n",
    "we need to decide where to perform this operation.\n",
    "For instance, as shown below,\n",
    "we can transfer `X` to the second accelerator\n",
    "and perform the operation there.\n",
    "Simply adding `X` and `Y` will error.\n",
    "The runtime engine would not know what to do:\n",
    "it cannot find data on the same device and it fails.\n",
    "Since `Y` lives on the second GPU,\n",
    "we need to move `X` there before we can add the two.\n",
    "\n",
    "![Copy data to perform an operation on the same device.](http://d2l.ai/_images/copyto.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "editable": true,
    "id": "0rHFiWiRA1tP",
    "outputId": "e19219b6-7ce5-4286-9180-7e329d8e6a91",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "X + Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "editable": true,
    "id": "8BmUl56EBCqp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To put it on a device, we will need to move it, which is again nothing stateful but functional: we get a moved tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72cffd2e",
    "origin_pos": 41,
    "outputId": "55904218-0620-4947-da98-e1e07440548a",
    "tab": [
     "jax"
    ]
   },
   "outputs": [],
   "source": [
    "Z = jax.device_put(X, try_tpu_gpu(1))\n",
    "print(X, X.devices())\n",
    "print(Z, Z.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "0e868fa0",
    "origin_pos": 42
   },
   "source": [
    "Now that the data (both `Z` and `Y`) are on the same GPU), we can add them up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54adc8d7",
    "origin_pos": 43,
    "outputId": "e7c30697-a177-42e1-8338-e6f40847748e",
    "tab": [
     "jax"
    ]
   },
   "outputs": [],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "bfd05286",
    "origin_pos": 47,
    "tab": [
     "jax"
    ]
   },
   "source": [
    "Imagine that your variable `Z` already lives on your second GPU.\n",
    "What happens if we still call `Z2 = Z` under the same device scope?\n",
    "It will return `Z` instead of making a copy and allocating new memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d48982e3",
    "origin_pos": 51,
    "outputId": "c94dc7bc-db1e-4f46-bffe-ef376e6c4896",
    "tab": [
     "jax"
    ]
   },
   "outputs": [],
   "source": [
    "Z2 = jax.device_put(Z, try_tpu_gpu(1))\n",
    "Z2 is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "editable": true,
    "id": "1f34545b",
    "origin_pos": 52,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The bottleneck\n",
    "\n",
    "Transferring variables between devices is slow: much slower than computation.\n",
    "So we want you to be 100% certain\n",
    "that you want to do something slow before we let you do it.\n",
    "If the deep learning framework just did the copy automatically\n",
    "without crashing then you might not realize\n",
    "that you had written some slow code.\n",
    "\n",
    "Transferring data is not only slow, it also makes parallelization a lot more difficult,\n",
    "since we have to wait for data to be sent (or rather to be received)\n",
    "before we can proceed with more operations.\n",
    "This is why copy operations should be taken with great care.\n",
    "As a rule of thumb, many small operations\n",
    "are much worse than one big operation.\n",
    "\n",
    "Last, when we print tensors or convert tensors to the NumPy format,\n",
    "if the data is not in the main memory,\n",
    "the framework will copy it to the main memory first,\n",
    "resulting in additional transmission overhead.\n",
    "Even worse, it is now subject to the dreaded global interpreter lock\n",
    "that makes everything wait for Python to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "editable": true,
    "id": "t3NmZMQTOyAX",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Lazy execution\n",
    "\n",
    "There is another trick that libraries may use to speed up the comutation: while the actual computation is performed, the Python code continues _until the value of the computation is needed_ (barrier).\n",
    "\n",
    "At this point, the Python code stopps until the needed computation is ready.\n",
    "Usually, we don't notice this (great! That's how it should be!), except if we're profiling code: when running something like `y = jax.something(x)`, then y is _technically_ not yet computed but a _lazy object_, a placeholder that says \"I am holding a value, I promise\".\n",
    "If we use Python that _needs_ this value (i.e. by calling `np.array(y)`, it will wait until the computation is finished.\n",
    "\n",
    "Therefore, to profile code, we're not interested in the time it takes to return a placeholder (which can be instant) but instead, how long it takes until the computation is performed.\n",
    "\n",
    "`block_until_ready()` forces the Python code to wait until the computation is executed (only usecase is benchmarking, should not be needed otherwise).\n",
    "\n",
    "_note that we cannot enforce the computation not to be executed, but that jax has the freedom to delay it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "editable": true,
    "id": "mmj8Ijx8QqZW",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "UuA4ACa5P7ku",
    "outputId": "bbc30c89-a3ba-4425-e4c2-9fadc4588f2a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "with jax.default_device(cpu()):\n",
    "    rand = random.randint(0, 1e4)\n",
    "    x = jnp.linspace(0, 100, num=10_000_000 + rand)\n",
    "    y = x ** 0.5 - jnp.log(jnp.abs(x));\n",
    "    # y.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "editable": true,
    "id": "9OV_th6LEA3w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 100_000_000\n",
    "# n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Pseudo random numbers\n",
    "\n",
    "How hard is it to get random numbers right?\n",
    "\n",
    "[Hard. JAX has an extensive, in-depth explanation for their choice of randomness](https://jax.readthedocs.io/en/latest/random-numbers.html)\n",
    "\n",
    "In short, we need to feed a seed/generator to every call (_before you complain, read the above. You may still complain, but not as loud_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "Z6HyAx0xCcqY",
    "outputId": "d59b6522-1263-4131-eaa3-1bc3368b5455",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n5\n",
    "x = jax.random.uniform(jax.random.PRNGKey(random.randint(0, 1e12)), (n,))  # on GPU/TPU\n",
    "xcpu = jax.device_put(x, cpu())  # move to CPU\n",
    "ycpu = xcpu ** 2 - 0.5  # calculate on CPU\n",
    "yacc = jax.device_put(ycpu, try_tpu_gpu())  # move to GPU/TPU\n",
    "zacc = yacc ** 3 - 0.2  # calculate on GPU/TPU\n",
    "zacc.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "Sxj0TNaSC-23",
    "outputId": "d20e3e66-62be-49db-cdad-146283b224d2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n5\n",
    "with jax.default_device(cpu()):\n",
    "    xcpu = jax.random.uniform(jax.random.PRNGKey(random.randint(0, 1e12)), (n,))  # on CPU\n",
    "    ycpu = xcpu ** 2 - 0.5\n",
    "    zcpu = ycpu ** 3 - 0.2\n",
    "    zcpu.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "16Ftya7REZvM"
   },
   "outputs": [],
   "source": [
    "%%timeit -n5\n",
    "with jax.default_device(try_tpu_gpu()):\n",
    "    xacc = jax.random.uniform(jax.random.PRNGKey(random.randint(0, 1e12)), (n,))  # on CPU\n",
    "    yacc = xacc ** 2 - 0.5\n",
    "    zacc = yacc ** 3 - 0.2\n",
    "    zacc.block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "As mentioned in the talk, TF and torch are similar. The concept is about the same, they all just have slightly different ways and syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "MZ0eOYNIDvsf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMSr66kVxOVd",
    "outputId": "350e0608-8d20-4d61-b5e6-bcb448a005c6"
   },
   "outputs": [],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "blv-i7GXOc67",
    "outputId": "f77aa770-9840-4ced-a740-97143d48b98b"
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('cpu'):\n",
    "    x = tnp.linspace(0, 10, num=100)\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "qKkGE6FMUdh6",
    "outputId": "7c7c34e9-96f1-4d0b-9793-555d7f7ec136"
   },
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrUbSCf5Ulpd",
    "outputId": "21f53b4c-f187-4792-eca4-0d839c863550"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(0., 10, 100).to('cpu')  # why I dislike torch: try to use the `num=100` -> fails.\n",
    "x.device                                   # torch is not very pythonic sometimes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "tvFA2Zg6xa0D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
